{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "applicable-retailer",
   "metadata": {},
   "source": [
    "# Download all the Current Preprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-ground",
   "metadata": {},
   "source": [
    "Download all the preprints in biorxiv and medrxiv so we can detect which tokens are changing through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-bahrain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:47.313417Z",
     "start_time": "2022-03-05T18:36:45.818040Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import plydata as ply\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-england",
   "metadata": {},
   "source": [
    "# BioRxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-abuse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:47.449858Z",
     "start_time": "2022-03-05T18:36:47.318656Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biorxiv_doc_hash_mapper_df = pd.read_csv(\n",
    "    \"output/biorxiv_doc_hash_mapper_updated.tsv\", sep=\"\\t\"\n",
    ")\n",
    "biorxiv_doc_hash_mapper_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-drain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:48.275072Z",
     "start_time": "2022-03-05T18:36:47.451521Z"
    }
   },
   "outputs": [],
   "source": [
    "already_parsed_folders = set(\n",
    "    [\n",
    "        Path(hash_file).parts[1:2][0]\n",
    "        for hash_file in set(biorxiv_doc_hash_mapper_df.hash.tolist())\n",
    "        if Path(hash_file).parts[1:2][0] != \"January_2020\"\n",
    "    ]\n",
    ")\n",
    "for folder in already_parsed_folders:\n",
    "    Path(f\"output/temp_batch_holder/{str(folder)}\").mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-plant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:48.648797Z",
     "start_time": "2022-03-05T18:36:48.276402Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_hash_mapper = set(\n",
    "    Path(hash_file).name for hash_file in biorxiv_doc_hash_mapper_df.hash.tolist()\n",
    ")\n",
    "new_doc_ids = set()\n",
    "new_doc_hash_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-afghanistan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:49.651740Z",
     "start_time": "2022-03-05T18:36:48.650092Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biorxiv_file_log = Path(\"output/biorxiv_batch_dirs.txt\")\n",
    "if not biorxiv_file_log.exists():\n",
    "    print(\"Please execute the following system commands line before continuing:\")\n",
    "    print(\n",
    "        \"s3cmd ls s3://biorxiv-src-monthly/Back_Content/* --requester-pays --recursive > output/biorxiv_batch_dirs_1.log\"\n",
    "    )\n",
    "    print(\n",
    "        \"s3cmd ls s3://biorxiv-src-monthly/Current_Content/* --requester-pays --recursive > output/biorxiv_batch_dirs_2.log\"\n",
    "    )\n",
    "    print(\"once the commands have finished please merge the files into one\")\n",
    "else:\n",
    "    with open(str(biorxiv_file_log), \"r\") as infile:\n",
    "        # doc_downloaded\n",
    "        for biorxiv_batch in infile:\n",
    "\n",
    "            biorxiv_batch = biorxiv_batch.strip()\n",
    "            biorxiv_batch_name = Path(biorxiv_batch).parts[2:]\n",
    "            batch_folder = Path(\n",
    "                f\"output/temp_batch_holder/biorxiv/{biorxiv_batch_name[1]}\"\n",
    "            )\n",
    "\n",
    "            if not batch_folder.exists():\n",
    "                print(f\"Downloading {str(biorxiv_batch)}\")\n",
    "                batch_folder.mkdir(exist_ok=True, parents=True)\n",
    "                result = subprocess.check_output(\n",
    "                    f\"s3cmd get {biorxiv_batch}* {batch_folder}/. --requester-pays\",\n",
    "                    shell=True,\n",
    "                )\n",
    "\n",
    "            for preprint_zipfile in tqdm.tqdm(batch_folder.rglob(\"*meca\")):\n",
    "                # Remove if already parsed\n",
    "                if preprint_zipfile.name in doc_hash_mapper:\n",
    "                    preprint_zipfile.unlink()\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with zipfile.ZipFile(preprint_zipfile) as infile:\n",
    "                        filename = list(\n",
    "                            filter(\n",
    "                                lambda x: str(Path(x).parent) == \"content\"\n",
    "                                and Path(x).suffix == \".xml\",\n",
    "                                infile.namelist(),\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        version_count = 1\n",
    "                        output_file_name = f\"{Path(filename[0]).name}_v{version_count}\"\n",
    "                        while output_file_name in new_doc_ids:\n",
    "                            version_count += 1\n",
    "                            output_file_name = (\n",
    "                                f\"{Path(filename[0]).stem}_v{version_count}\"\n",
    "                            )\n",
    "\n",
    "                        new_doc_ids.add(output_file_name)\n",
    "\n",
    "                        article_file = Path(\n",
    "                            f\"output/biorxiv_medrxiv_dump/biorxiv/{output_file_name}.xml\"\n",
    "                        )\n",
    "\n",
    "                        new_doc_hash_rows.append(\n",
    "                            {\n",
    "                                \"hash\": f\"{'/'.join(biorxiv_batch_name)}/{preprint_zipfile.name}\",\n",
    "                                \"doc_number\": str(article_file.name),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        # Write new preprint to file\n",
    "                        with infile.open(str(filename[0])) as doc_xml, open(\n",
    "                            str(article_file), \"w\"\n",
    "                        ) as outfile:\n",
    "                            outfile.write(doc_xml.read().decode(\"utf-8\"))\n",
    "\n",
    "                        preprint_zipfile.unlink()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f\"Email biorxiv about this hash: {preprint_zipfile}\")\n",
    "                    preprint_zipfile.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-clerk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:49.660515Z",
     "start_time": "2022-03-05T18:36:49.654707Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(new_doc_hash_rows) > 0:\n",
    "    (\n",
    "        biorxiv_doc_hash_mapper_df\n",
    "        >> ply.call(\".append\", pd.DataFrame.from_records(new_doc_hash_rows))\n",
    "        >> ply.call(\".reset_index\")\n",
    "        >> ply.select(\"-index\")\n",
    "        >> ply.call(\n",
    "            \".to_csv\",\n",
    "            \"output/biorxiv_doc_hash_mapper_updated.tsv\",\n",
    "            sep=\"\\t\",\n",
    "            index=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-instrumentation",
   "metadata": {},
   "source": [
    "# MedRxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-chicken",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:49.708753Z",
     "start_time": "2022-03-05T18:36:49.663632Z"
    }
   },
   "outputs": [],
   "source": [
    "if Path(\"output/medrxiv_doc_hash_mapper_updated.tsv\").exists():\n",
    "    medrxiv_doc_hash_mapper_df = pd.read_csv(\n",
    "        \"output/medrxiv_doc_hash_mapper_updated.tsv\", sep=\"\\t\"\n",
    "    )\n",
    "else:\n",
    "    medrxiv_doc_hash_mapper_df = pd.DataFrame([], columns=[\"hash\", \"doc_number\"])\n",
    "medrxiv_doc_hash_mapper_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-exhibit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:49.764688Z",
     "start_time": "2022-03-05T18:36:49.710506Z"
    }
   },
   "outputs": [],
   "source": [
    "already_parsed_folders = set(\n",
    "    [\n",
    "        Path(hash_file).parts[1:2][0]\n",
    "        for hash_file in set(medrxiv_doc_hash_mapper_df.hash.tolist())\n",
    "    ]\n",
    ")\n",
    "for folder in already_parsed_folders:\n",
    "    Path(f\"output/temp_batch_holder/medrxiv/{str(folder)}\").mkdir(\n",
    "        exist_ok=True, parents=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-arrival",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:36:49.828800Z",
     "start_time": "2022-03-05T18:36:49.766050Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_hash_mapper = set(\n",
    "    Path(hash_file).name for hash_file in medrxiv_doc_hash_mapper_df.hash.tolist()\n",
    ")\n",
    "new_doc_ids = set()\n",
    "new_doc_hash_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-spice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T20:35:54.850013Z",
     "start_time": "2022-03-05T18:36:49.830106Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "medrxiv_file_log = Path(\"output/medrxiv_batch_dirs.txt\")\n",
    "if not medrxiv_file_log.exists():\n",
    "    print(\"Please execute the following system commands line before continuing:\")\n",
    "    print(\n",
    "        \"s3cmd ls s3://medrxiv-src-monthly/Back_Content/* --requester-pays --recursive > output/medrxiv_batch_dirs_1.txt\"\n",
    "    )\n",
    "    print(\n",
    "        \"s3cmd ls s3://medrxiv-src-monthly/Current_Content/* --requester-pays --recursive > output/medrxiv_batch_dirs_2.txt\"\n",
    "    )\n",
    "    print(\"once the commands have finished please merge the files into one\")\n",
    "else:\n",
    "    with open(str(medrxiv_file_log), \"r\") as infile:\n",
    "        # doc_downloaded\n",
    "        for medrxiv_batch in infile:\n",
    "\n",
    "            medrxiv_batch = medrxiv_batch.strip()\n",
    "            medrxiv_batch_name = Path(medrxiv_batch).parts[2:]\n",
    "            batch_folder = Path(\n",
    "                f\"output/temp_batch_holder/medrxiv/{medrxiv_batch_name[1]}\"\n",
    "            )\n",
    "\n",
    "            if not batch_folder.exists():\n",
    "                print(f\"Downloading {str(medrxiv_batch)}\")\n",
    "                batch_folder.mkdir(exist_ok=True, parents=True)\n",
    "                result = subprocess.check_output(\n",
    "                    f\"s3cmd get {medrxiv_batch}* {batch_folder}/. --requester-pays\",\n",
    "                    shell=True,\n",
    "                )\n",
    "\n",
    "            for preprint_zipfile in tqdm.tqdm(batch_folder.rglob(\"*meca\")):\n",
    "                # Remove if already parsed\n",
    "                if preprint_zipfile.name in doc_hash_mapper:\n",
    "                    preprint_zipfile.unlink()\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with zipfile.ZipFile(preprint_zipfile) as infile:\n",
    "                        filename = list(\n",
    "                            filter(\n",
    "                                lambda x: str(Path(x).parent) == \"content\"\n",
    "                                and Path(x).suffix == \".xml\",\n",
    "                                infile.namelist(),\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        version_count = 1\n",
    "                        output_file_name = f\"{Path(filename[0]).stem}_v{version_count}\"\n",
    "                        while output_file_name in new_doc_ids:\n",
    "                            version_count += 1\n",
    "                            output_file_name = (\n",
    "                                f\"{Path(filename[0]).stem}_v{version_count}\"\n",
    "                            )\n",
    "\n",
    "                        new_doc_ids.add(output_file_name)\n",
    "\n",
    "                        article_file = Path(\n",
    "                            f\"output/biorxiv_medrxiv_dump/medrxiv/{output_file_name}.xml\"\n",
    "                        )\n",
    "\n",
    "                        new_doc_hash_rows.append(\n",
    "                            {\n",
    "                                \"hash\": f\"{'/'.join(medrxiv_batch_name)}/{preprint_zipfile.name}\",\n",
    "                                \"doc_number\": str(article_file.name),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        # Write new preprint to file\n",
    "                        with infile.open(str(filename[0])) as doc_xml, open(\n",
    "                            str(article_file), \"w\"\n",
    "                        ) as outfile:\n",
    "                            outfile.write(doc_xml.read().decode(\"utf-8\"))\n",
    "\n",
    "                        preprint_zipfile.unlink()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f\"Email medrxiv about this hash: {preprint_zipfile}\")\n",
    "                    preprint_zipfile.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-undergraduate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T20:35:54.949672Z",
     "start_time": "2022-03-05T20:35:54.851214Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(new_doc_hash_rows) > 0:\n",
    "    (\n",
    "        medrxiv_doc_hash_mapper_df\n",
    "        >> ply.call(\".append\", pd.DataFrame.from_records(new_doc_hash_rows))\n",
    "        >> ply.call(\".reset_index\")\n",
    "        >> ply.select(\"-index\")\n",
    "        >> ply.call(\n",
    "            \".to_csv\",\n",
    "            \"output/medrxiv_doc_hash_mapper_updated.tsv\",\n",
    "            sep=\"\\t\",\n",
    "            index=False,\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:biovectors]",
   "language": "python",
   "name": "conda-env-biovectors-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
