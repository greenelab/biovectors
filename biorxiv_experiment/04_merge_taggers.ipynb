{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge GnormPlus and Tagger One Tags into One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tagging system used a divide and conquer approach, where each tagger tagged every preprint for their own respective entities.\n",
    "This notebook is designed to merge them back into one single file.\n",
    "Basically it aligns the passages and checks to see which entity comes first if present and then merges them into one single passage block.\n",
    "Once this process is finished preprints are ready to be processed by the main data analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:42:02.935567Z",
     "start_time": "2022-05-01T21:42:02.916502Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import lxml.etree as ET\n",
    "from lxml.etree import XMLSyntaxError\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:42:02.952716Z",
     "start_time": "2022-05-01T21:42:02.940935Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = Path(\"output/final\")\n",
    "gnormplus_folder = Path(\"output/gnormplus_tags\")\n",
    "tagger_one_folder = Path(\"output/tagger_one_tags/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:47:28.064857Z",
     "start_time": "2022-05-01T21:42:02.958421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015:: 1059it [00:00, 2856.94it/s]\n",
      "2019:: 31774it [00:32, 978.66it/s] \n",
      "2018:: 10130it [00:18, 559.39it/s]\n",
      "2017:: 5872it [00:11, 519.86it/s]\n",
      "2022:: 8531it [00:52, 163.98it/s]\n",
      "2020:: 52846it [02:46, 317.89it/s]\n",
      "2021:: 54870it [00:34, 1589.44it/s]\n",
      "2014:: 486it [00:05, 89.06it/s] \n",
      "2016:: 2410it [00:03, 629.57it/s]\n",
      "2013:: 60it [00:00, 81.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for gnormplus_folder_year in gnormplus_folder.glob(\"*\"):\n",
    "\n",
    "    for gnormplus_doc_path in tqdm.tqdm(\n",
    "        gnormplus_folder_year.rglob(\"*xml\"), desc=f\"{gnormplus_folder_year.stem}\"\n",
    "    ):\n",
    "        final_doc_name = (\n",
    "            f\"{output_folder}/{gnormplus_folder_year.stem}/{gnormplus_doc_path.name}\"\n",
    "        )\n",
    "\n",
    "        if Path(final_doc_name).exists():\n",
    "            continue\n",
    "\n",
    "        gnormplus_doc = ET.parse(str(gnormplus_doc_path)).getroot()\n",
    "        try:\n",
    "            tagger_one_doc = ET.parse(\n",
    "                f\"{tagger_one_folder}/{gnormplus_folder_year.stem}/{gnormplus_doc_path.name}\"\n",
    "            ).getroot()\n",
    "        except XMLSyntaxError:\n",
    "            tagger_one_doc = ET.Element(\"document\")\n",
    "\n",
    "        merged_doc = ET.Element(\"document\")\n",
    "        merged_doc.append(gnormplus_doc.xpath(\"document/id\")[0])  # Add the doc id\n",
    "\n",
    "        tagger_one_passages = tagger_one_doc.xpath(\"document/passage\")\n",
    "        gnormplus_passages = gnormplus_doc.xpath(\"document/passage\")\n",
    "\n",
    "        running_annotation_id = 0\n",
    "\n",
    "        if len(tagger_one_passages) == 0:\n",
    "            for gnormplus_passage in gnormplus_passages:\n",
    "                total_annotations = gnormplus_passage.xpath(\"annotation\")\n",
    "\n",
    "                # if no annotations add the passage and continue\n",
    "                if len(total_annotations) == 0:\n",
    "                    merged_doc.append(gnormplus_passage)\n",
    "                    continue\n",
    "\n",
    "                sorted(\n",
    "                    total_annotations,\n",
    "                    key=lambda x: int(x.xpath(\"location\")[0].attrib[\"offset\"]),\n",
    "                )\n",
    "\n",
    "                new_passage = ET.Element(\"passage\")\n",
    "                new_passage.extend(\n",
    "                    [gnormplus_passage[0], gnormplus_passage[1], gnormplus_passage[2]]\n",
    "                )\n",
    "                for annotation in total_annotations:\n",
    "                    annotation.attrib[\"id\"] = str(running_annotation_id)\n",
    "                    new_passage.append(annotation)\n",
    "                    running_annotation_id += 1\n",
    "\n",
    "                merged_doc.append(new_passage)\n",
    "\n",
    "        elif len(gnormplus_passages) == 0:\n",
    "            for tagger_one_passage in tagger_one_passages:\n",
    "                total_annotations = tagger_one_passage.xpath(\"annotation\")\n",
    "\n",
    "                # if no annotations add the passage and continue\n",
    "                if len(total_annotations) == 0:\n",
    "                    merged_doc.append(tagger_one_passage)\n",
    "                    continue\n",
    "\n",
    "                sorted(\n",
    "                    total_annotations,\n",
    "                    key=lambda x: int(x.xpath(\"location\")[0].attrib[\"offset\"]),\n",
    "                )\n",
    "\n",
    "                new_passage = ET.Element(\"passage\")\n",
    "                new_passage.extend(\n",
    "                    [\n",
    "                        tagger_one_passage[0],\n",
    "                        tagger_one_passage[1],\n",
    "                        tagger_one_passage[2],\n",
    "                    ]\n",
    "                )\n",
    "                for annotation in total_annotations:\n",
    "                    annotation.attrib[\"id\"] = str(running_annotation_id)\n",
    "                    new_passage.append(annotation)\n",
    "                    running_annotation_id += 1\n",
    "\n",
    "                merged_doc.append(new_passage)\n",
    "        else:\n",
    "            passage_merge_generator = zip(tagger_one_passages, gnormplus_passages)\n",
    "            for tagger_one_passage, gnormplus_passage in passage_merge_generator:\n",
    "                total_annotations = tagger_one_passage.xpath(\n",
    "                    \"annotation\"\n",
    "                ) + gnormplus_passage.xpath(\"annotation\")\n",
    "\n",
    "                # if no annotations add the passage and continue\n",
    "                if len(total_annotations) == 0:\n",
    "                    merged_doc.append(gnormplus_passage)\n",
    "                    continue\n",
    "\n",
    "                sorted(\n",
    "                    total_annotations,\n",
    "                    key=lambda x: int(x.xpath(\"location\")[0].attrib[\"offset\"]),\n",
    "                )\n",
    "\n",
    "                new_passage = ET.Element(\"passage\")\n",
    "                new_passage.extend(\n",
    "                    [gnormplus_passage[0], gnormplus_passage[1], gnormplus_passage[2]]\n",
    "                )\n",
    "                for annotation in total_annotations:\n",
    "                    annotation.attrib[\"id\"] = str(running_annotation_id)\n",
    "                    annotation[0].attrib[\"key\"] = \"identifier\"\n",
    "                    new_passage.append(annotation)\n",
    "                    running_annotation_id += 1\n",
    "\n",
    "                merged_doc.append(new_passage)\n",
    "\n",
    "        doc_header = ET.Element(\"collection\")\n",
    "        doc_header.extend([gnormplus_doc[0], gnormplus_doc[1], gnormplus_doc[2]])\n",
    "        doc_header.append(merged_doc)\n",
    "        Path(f\"{output_folder}/{gnormplus_folder_year.stem}\").mkdir(\n",
    "            exist_ok=True, parents=True\n",
    "        )\n",
    "        ET.ElementTree(doc_header).write(\n",
    "            final_doc_name, pretty_print=True, method=\"c14n\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:biovectors]",
   "language": "python",
   "name": "conda-env-biovectors-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
